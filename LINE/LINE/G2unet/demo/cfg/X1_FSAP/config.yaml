run_dir: '.runs'
run_name: 'unet2d'

epochs: 100
cuda_devices: '0'
seed: 42  # random seed

# training displaying
display_freq: 2 # display image
eval_freq: 2  # after eval_freq epoch, validate data
save_freq: 0 # if savestep==0, don't save

# changes path when swithes betwen train and test
checkpoint: ''

use_background_channel: false # 1- \sum
transform_params: # data_aug
    rotate_rate: 0.1
    angle: 2
    flip_rate: 0
    axis: 1
    translate_rate: 0.1
    offsets: [10,10]

mix_step: 0 # mix_training batch step: 0 for no mix

dataset:
    name_list: [ 'cephalometric' ]
    loss_weights: [1, 1, 1, ]
    batch_size_dic: # this will override that of data_loader in train stage
        hand: 4
        chest: 4
        cephalometric: 4

    cephalometric:
    # spacing: 0.1 x 0.1 mm
        prefix: './G2unet/data/ISBI2015_ceph'
        sigma: 3
        num_landmark: 36
        size: [416, 512] # resize to width x height, origin_size: [1935,2400]

dataloader:
    train:
        batch_size: 1
        # num_workers: 0
        shuffle: true
        drop_last: true
    validate:
        batch_size: 1
        # num_workers: 1
        shuffle: true
        drop_last: true
    test:
        batch_size: 1
        # num_workers: 1
        shuffle: true

hand_net:
    in_channels: 1
    out_channels: 37

cephalometric_net:
    in_channels: 1
    out_channels: 36
chest_net:
    in_channels: 1
    out_channels: 6
model: 'unet2d'  # which model to use
gln:
    localNet: 'unet2d'
    globalNet_params:
        scale_factor: 0.25
        kernel_size: 3
        dilations: [1, 2, 5, 2, 1]
gln2:
    localNet: 'unet2d'
    globalNet_params:
        scale_factor: 0.25
        kernel_size: 3
        dilations: [1, 2, 5, 2, 1]
learning:
    loss: 'bce'
    l1:
        reduction: 'sum'
    l2:
        reduction: 'sum'
    bce:
        reduction: 'sum'
    optim: 'adam'
    adam:
        lr: 0.0001
        weight_decay: 0.0001 # 0.00001
    use_scheduler: true
    scheduler: 'cycliclr'
    steplr:
        step_size: 30
        gamma: 0.5
    cycliclr:
        base_lr: 0.0001 # 0.00008
        max_lr: 0.01 # 0.008
        step_size_up: 2000 # step_size_up = len(dataset)/batch_size * (2~10)
        step_size_down: 2000 # same as step_size_up
        mode: 'triangular2'
        cycle_momentum: false
